import numpy as np 
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures 
from sklearn.metrics import mean_squared_error, r2_score
%matplotlib inline

d1, d2, d3, d4 = 5, 10, 25, 100
D = d2
x = (np.random.rand(D, 1)-0.5) * 20
xt = np.ones((1,D))
f = x[:,0]

k1, k2, k3 = 1, 4, 16
K = k2
W = np.ones((K+1, 1))
for i in range(1, K+1):
    xt = np.append(xt, f**i)
XT = xt.reshape(K+1, D)
X = np.transpose(XT)
Y = np.dot(X,W)
Y_norm = Y + np.random.normal(scale=3, size=Y.shape)
print('X=', X, 'Coeff=', W, 'Y=', Y, 'Y_norm=', Y_norm)

mse = mean_squared_error(Y_norm, h)
rmse = np.sqrt(mean_squared_error(Y_norm, h))
r2 = r2_score(Y_norm, h)
print('MSE of model', mse) 
print('RMSE of model', rmse)  
print('R2 score of model: ', r2)

poly_features = PolynomialFeatures(degree = K, include_bias = False) 
x_poly = poly_features.fit_transform(x) 
lin_reg = LinearRegression() 
lin_reg.fit(x_poly, Y_norm) 
print('Coefficients of x are', lin_reg.coef_) 
print('Intercept is', lin_reg.intercept_)
x_new = np.linspace(-10, 10, D).reshape(D, 1) 
x_new_poly = poly_features.transform(x_new) 
y_new = lin_reg.predict(x_new_poly) 
plt.plot(x, Y_norm, "b.") 
plt.plot(x_new, y_new, "r-", linewidth = 2, label ="Predictions") 
plt.xlabel("$x$", fontsize = 18) 
plt.ylabel("$y$", rotation = 0, fontsize = 18) 
plt.legend(loc ="upper left", fontsize = 14)   
plt.title("Non Linear Prediction")

plt.show()
